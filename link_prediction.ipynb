{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd7916a-9d78-413e-9812-d491538dba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.10.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import the NetworkX package\n",
    "import networkx as nx\n",
    "import torch\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2ccca7-b86a-4b7b-b391-1cf722eb3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/train.csv\")\n",
    "test_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/test.csv\")\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/upload.csv\")\n",
    "content_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/content.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85d4f63-d64d-4f9f-bc83-d3138c9d7ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: 8686\n",
      "       id    to  from  label\n",
      "0  E10311  2399  2339      0\n",
      "1  E10255  2397  1144      1\n",
      "2  E10667   854  1726      0\n",
      "3   E9395   872   702      0\n",
      "4   E5926  2450  1312      1\n",
      "testing data: 2172\n",
      "       id    to  from\n",
      "0  E10559  2323  2673\n",
      "1   E4849    81  1634\n",
      "2   E3964  2405  1765\n",
      "3    E542  2114   498\n",
      "4    E331  1013   849\n",
      "upload data: 2172\n",
      "       id  prob\n",
      "0  E10559   0.5\n",
      "1   E4849   0.5\n",
      "2   E3964   0.5\n",
      "3    E542   0.5\n",
      "4    E331   0.5\n",
      "content data: 2708\n"
     ]
    }
   ],
   "source": [
    "print(\"training data:\", len(train_csv))\n",
    "print(train_csv.head())\n",
    "print(\"testing data:\", len(test_csv))\n",
    "print(test_csv.head())\n",
    "print(\"upload data:\", len(upload_csv))\n",
    "print(upload_csv.head())\n",
    "print(\"content data:\", len(content_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772df38b-10e3-4d6c-a673-bf0fbf7664d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd21a678-c83e-44ea-bb69-7f16685fa36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>E9183</td>\n",
       "      <td>816</td>\n",
       "      <td>1769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>E5420</td>\n",
       "      <td>816</td>\n",
       "      <td>2461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   to  from  label\n",
       "24    E9183  816  1769      0\n",
       "3942  E5420  816  2461      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv[train_csv['to'] ==816]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4042c4-0472-417a-9cd8-2e353fa985a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 2339)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv['node_pair'] = train_csv[['to', 'from']].apply(tuple, axis=1)\n",
    "train_csv['node_pair'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2d7c13-140c-43dd-ae2c-a43994b596dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_csv data: 4324\n"
     ]
    }
   ],
   "source": [
    "edge_csv = train_csv[train_csv[\"label\"]==1]\n",
    "print(\"edge_csv data:\", len(edge_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64944cb1-83a0-434e-8c94-c17df1f520e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7472)\n",
      "torch.Size([2994, 3703])\n",
      "3703\n"
     ]
    }
   ],
   "source": [
    "class ReadData:\n",
    "    def __init__(self, dataset):\n",
    "        self.__edges = list()\n",
    "        self.__feature = dict()\n",
    "        \n",
    "        train_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/{}/train.csv\".format(dataset))\n",
    "        test_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/{}/test.csv\".format(dataset))\n",
    "        content_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/{}/content.csv\".format(dataset), header=None)\n",
    "        edge_csv = train_csv[train_csv[\"label\"]==1]\n",
    "        train_csv['node_pair'] = train_csv[['to', 'from']].apply(tuple, axis=1)\n",
    "        test_csv['node_pair'] = test_csv[['to', 'from']].apply(tuple, axis=1)\n",
    "        \n",
    "        # get feature\n",
    "        for line in content_csv.to_numpy():\n",
    "            l = line[0].split(\"\\t\")\n",
    "            self.__feature[int(l[0])] = list(map(int,l[1:]))\n",
    "        \n",
    "        # Create an undirected graph G\n",
    "        # read node pair\n",
    "        a_list, b_list, Gtext = [], [], []\n",
    "        for line in edge_csv.to_numpy():\n",
    "            a_list.append(line[1])\n",
    "            b_list.append(line[2])\n",
    "            Gtext.append((line[1], line[2]))\n",
    "        self.__edges = [a_list+b_list, b_list+a_list]\n",
    "        #self.__edges = [a_list, b_list]\n",
    "        \n",
    "        \n",
    "        self.__Graph = nx.Graph(Gtext)\n",
    "        self.__nodes_list = [self.__feature[node] for node in self.__Graph.nodes]\n",
    "        \n",
    "        # get common neighbors\n",
    "        _, common_neighbors = self.get_common_neighbors(train_csv['node_pair'].values)\n",
    "        _, test_common_neighbors = self.get_common_neighbors(test_csv['node_pair'].values)\n",
    "        train_csv[\"ComNei\"] = common_neighbors\n",
    "        test_csv[\"ComNei\"] = test_common_neighbors\n",
    "        \n",
    "        # get Preferential Attachment\n",
    "        _, preatt = self.get_preatt(train_csv['node_pair'].values)\n",
    "        _, test_preatt = self.get_preatt(test_csv['node_pair'].values)\n",
    "        train_csv[\"PreAtt\"] = preatt\n",
    "        test_csv[\"PreAtt\"] = test_preatt\n",
    "        \n",
    "        # get Jaccard's Coefficient\n",
    "        _, jaccard = self.get_jaccard(train_csv['node_pair'].values)\n",
    "        _, test_jaccard = self.get_jaccard(test_csv['node_pair'].values)\n",
    "        train_csv[\"Jaccard\"] = jaccard\n",
    "        test_csv[\"Jaccard\"] = test_jaccard\n",
    "        \n",
    "        # get Adamic/Adar\n",
    "        _, aa = self.get_aa(train_csv['node_pair'].values)\n",
    "        _, test_aa = self.get_aa(test_csv['node_pair'].values)\n",
    "        train_csv[\"AA\"] = aa\n",
    "        test_csv[\"AA\"] = test_aa\n",
    "        \n",
    "#         # get Salton index\n",
    "#         _, salton = self.get_salton(train_csv['node_pair'].values)\n",
    "#         _, test_salton = self.get_salton(test_csv['node_pair'].values)\n",
    "#         train_csv[\"Salton\"] = salton\n",
    "#         test_csv[\"Salton\"] = test_salton\n",
    "        \n",
    "#         # get SÎ¦rensen index\n",
    "#         _, sphi = self.get_sphi(train_csv['node_pair'].values)\n",
    "#         _, test_sphi = self.get_sphi(test_csv['node_pair'].values)\n",
    "#         train_csv[\"Sphi\"] = sphi\n",
    "#         test_csv[\"Sphi\"] = test_sphi\n",
    "        \n",
    "#         # get hub promoted index\n",
    "#         _, hub = self.get_hubpro(train_csv['node_pair'].values)\n",
    "#         _, test_hub = self.get_hubpro(test_csv['node_pair'].values)\n",
    "#         train_csv[\"HubPro\"] = hub\n",
    "#         test_csv[\"HubPro\"] = test_hub\n",
    "        \n",
    "#         # get hub depressed index\n",
    "#         _, hub = self.get_hubdep(train_csv['node_pair'].values)\n",
    "#         _, test_hub = self.get_hubdep(test_csv['node_pair'].values)\n",
    "#         train_csv[\"HubDep\"] = hub\n",
    "#         test_csv[\"HubDep\"] = test_hub\n",
    "        \n",
    "#         # get Leicht-Holme-Newman Index\n",
    "#         _, leicht = self.get_leicht(train_csv['node_pair'].values)\n",
    "#         _, test_leicht = self.get_leicht(test_csv['node_pair'].values)\n",
    "#         train_csv[\"Leicht\"] = leicht\n",
    "#         test_csv[\"Leicht\"] = test_leicht\n",
    "        \n",
    "#         # get Resource allocation\n",
    "#         _, res = self.get_resource(train_csv['node_pair'].values)\n",
    "#         _, test_res = self.get_resource(test_csv['node_pair'].values)\n",
    "#         train_csv[\"Res\"] = res\n",
    "#         test_csv[\"Res\"] = test_res\n",
    "        \n",
    "        # # get Random walk with restart\n",
    "        # edge_file_name = \"edge_{}.txt\".format(dataset)\n",
    "        # self.write_txt(edge_file_name)\n",
    "        # train_csv[\"Rwr\"] = self.get_rwr_scores(train_csv, edge_file_name)\n",
    "        # test_csv[\"Rwr\"] =self.get_rwr_scores(test_csv, edge_file_name)\n",
    "        \n",
    "            \n",
    "        # print(\"training dataframe: \")\n",
    "        # print(train_csv.sort_values(by=\"Rwr\", ascending=False).head())\n",
    "        # print(\"\\ntesting dataframe: \")\n",
    "        # print(test_csv.sort_values(by=\"Rwr\", ascending=False).head())\n",
    "        self.__data_array = train_csv.to_numpy()\n",
    "        self.__test_array = test_csv.to_numpy()\n",
    "    \n",
    "    def get_nodes(self):\n",
    "        return torch.Tensor(self.__nodes_list)\n",
    "\n",
    "    def get_edges(self):\n",
    "        return np.array(self.__edges)\n",
    "    \n",
    "    def get_torch_edges(self):\n",
    "        return torch.tensor(self.__edges, dtype=torch.long)\n",
    "    \n",
    "    def get_nodes_x(self):\n",
    "        features = self.get_feature()\n",
    "        training_array = [features[i] for i in range(len(features.keys()))]\n",
    "        \n",
    "        return torch.tensor(training_array, dtype=torch.float)\n",
    "    \n",
    "        \n",
    "    def get_num_feature(self):\n",
    "        return self.__data_array[:, 5:].shape[1]\n",
    "    \n",
    "    def get_feature(self):\n",
    "        return self.__feature\n",
    "    \n",
    "    def get_common_neighbors(self, node_pairs):\n",
    "        common_neighbors = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                common_neighbors.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            common_neighbors.append(len(set(to_nlist)&set(from_nlist)))\n",
    "            \n",
    "        return torch.Tensor(common_neighbors), common_neighbors\n",
    "    \n",
    "    # Preferential Attachment\n",
    "    def get_preatt(self, node_pairs):\n",
    "        att = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                att.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            att.append(len(to_nlist) * len(from_nlist))\n",
    "            \n",
    "        return  torch.Tensor(att), att\n",
    "    \n",
    "    # Jaccard's Coeffieient\n",
    "    def get_jaccard(self, node_pairs):\n",
    "        jaccard = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                jaccard.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            jaccard.append(round(len(set(to_nlist)&set(from_nlist)) / len(set(to_nlist)|set(from_nlist)), 3))\n",
    "            \n",
    "        return torch.Tensor(jaccard), jaccard\n",
    "    \n",
    "    # Adamic/Adar\n",
    "    def get_aa(self, node_pairs):\n",
    "        aa = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                aa.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            neighbors = set(to_nlist)&set(from_nlist)\n",
    "            aa_score = 0\n",
    "            if len(neighbors) != 0:\n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_friends = [i for i in self.__Graph.neighbors(neighbor)]\n",
    "                    if len(neighbor_friends)==1:\n",
    "                        continue\n",
    "                    aa_score += 1/ math.log(len(neighbor_friends))\n",
    "            aa.append(round(aa_score, 3))\n",
    "            \n",
    "        return torch.Tensor(aa), aa\n",
    "    \n",
    "    # Salton\n",
    "    def get_salton(self, node_pairs):\n",
    "        salton = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                salton.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            salton.append(round(len(set(to_nlist)&set(from_nlist)) / (len(set(to_nlist))*len(set(from_nlist))) ** 0.5, 3))\n",
    "            \n",
    "        return torch.Tensor(salton), salton \n",
    "    \n",
    "    # SÎ¦rensen index\n",
    "    def get_sphi(self, node_pairs):\n",
    "        sphi = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                sphi.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            sphi.append(round(len(set(to_nlist)&set(from_nlist)) * 2 / (len(set(to_nlist)) + len(set(from_nlist))), 3))\n",
    "            \n",
    "        return torch.Tensor(sphi), sphi \n",
    "    \n",
    "    # Hub Promoted\n",
    "    def get_hubpro(self, node_pairs):\n",
    "        hub = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                hub.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            hub.append(round(len(set(to_nlist)&set(from_nlist)) * 2 / min(len(set(to_nlist)), len(set(from_nlist))), 3))\n",
    "            \n",
    "        return torch.Tensor(hub), hub \n",
    "    \n",
    "    # Hub Depressed\n",
    "    def get_hubdep(self, node_pairs):\n",
    "        hub = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                hub.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            hub.append(round(len(set(to_nlist)&set(from_nlist)) * 2 / max(len(set(to_nlist)), len(set(from_nlist))), 3))\n",
    "            \n",
    "        return torch.Tensor(hub), hub \n",
    "    \n",
    "    # Leicht-Holme-Newman\n",
    "    def get_leicht(self, node_pairs):\n",
    "        leicht = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                leicht.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            leicht.append(round(len(set(to_nlist)&set(from_nlist)) / (len(set(to_nlist))*len(set(from_nlist))), 3))\n",
    "            \n",
    "        return torch.Tensor(leicht), leicht \n",
    "    \n",
    "    # Resource allocation\n",
    "    def get_resource(self, node_pairs):\n",
    "        res = list()\n",
    "        for i, j in node_pairs:\n",
    "            if i not in self.__Graph.nodes or j not in self.__Graph.nodes:\n",
    "                res.append(0)\n",
    "                continue\n",
    "            to_nlist = [ n for n in self.__Graph.neighbors(i)]\n",
    "            from_nlist = [ n for n in self.__Graph.neighbors(j)]\n",
    "            neighbors = set(to_nlist)&set(from_nlist)\n",
    "            res_score = 0\n",
    "            if len(neighbors) != 0:\n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_friends = [i for i in self.__Graph.neighbors(neighbor)]\n",
    "                    if len(neighbor_friends)==1:\n",
    "                        continue\n",
    "                    res_score += 1/ len(neighbor_friends)\n",
    "            res.append(round(res_score, 3))\n",
    "            \n",
    "        return torch.Tensor(res), res\n",
    "    \n",
    "    def get_training_data(self):\n",
    "        target = self.__data_array[:, 3]\n",
    "        training_array = self.__data_array[:, 5:].astype(float)\n",
    "        training_array = self.normalize(training_array)\n",
    "        \n",
    "        return training_array, target\n",
    "    \n",
    "    def get_testing_data(self):\n",
    "        testing_array = self.__test_array[:, 4:].astype(float)\n",
    "        testing_array = self.normalize(testing_array)\n",
    "        \n",
    "        return testing_array\n",
    "    \n",
    "    def normalize(self, dataset):\n",
    "        mean = np.mean(dataset, axis=0)\n",
    "        std  = np.std(dataset, axis=0)\n",
    "        \n",
    "        return (dataset - mean)/std\n",
    "    \n",
    "    def get_graph(self):\n",
    "        return self.__Graph\n",
    "    \n",
    "    # Random Work with restart\n",
    "    def get_rwr_scores(self, df, edge_file_name):\n",
    "        rrwr = RWR()\n",
    "        rrwr.read_graph(\"./{}\".format(edge_file_name), \"undirected\")\n",
    "        max_size, start_node = max(self.__Graph.nodes) + 1, min(self.__Graph.nodes)\n",
    "        edges = self.get_edges()\n",
    "        rwr_score = np.zeros((len(df), 1))\n",
    "        for i in tqdm(range(start_node, max_size), desc=\"rwr scores..\"):\n",
    "            t = df[df['from']==i]\n",
    "            if t.empty is False:\n",
    "                df_idx = t.index\n",
    "                # i_scores = rrwr.compute(seed=i)\n",
    "                i_scores = rwr(edges, i, max_size, alpha = 0.85, max_iter = 100)\n",
    "                for idx, j in enumerate(t.to.values):\n",
    "                    # j -= start_node\n",
    "                    rwr_score[idx] = np.round(i_scores[j], 3)\n",
    "        \n",
    "        return rwr_score\n",
    "    \n",
    "    def write_txt(self, output_path):\n",
    "        f = open(output_path, \"w\")\n",
    "        for a, b in self.__Graph.edges:\n",
    "            f.write(f\"{a}\\t{b}\\n\")\n",
    "        f.close()\n",
    "        \n",
    "    def get_test_nodepairs(self):\n",
    "        node_pairs = self.__test_array[:, 1:3].astype(int)\n",
    "        \n",
    "        return torch.tensor(node_pairs.T, dtype=torch.long)\n",
    "    \n",
    "    def get_training_nodepairs(self):\n",
    "        node_pairs = self.__data_array[:, 1:3].astype(int)\n",
    "        \n",
    "        return torch.tensor(node_pairs.T, dtype=torch.long)\n",
    "          \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"./pyrwr/\"))\n",
    "from pyrwr.rwr import RWR\n",
    "data = ReadData(\"dataset2\")\n",
    "print(data.get_edges().shape)\n",
    "print(data.get_nodes().shape)\n",
    "#print(data.get_common_neighbors())\n",
    "print(len(data.get_feature()[351]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e6b8f-ff6f-457f-9f32-97e4fb427db2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DirectedGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "02630586-cfa6-435b-bd42-0856b65897e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 877/877 [00:11<00:00, 82.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing dataframe: \n",
      "        id   to  from           Rwr\n",
      "5    E1391  117   793  1.185581e-04\n",
      "4    E2161  466   199  8.880946e-08\n",
      "2    E3190  739   468  2.318347e-10\n",
      "0     E370   26   317  0.000000e+00\n",
      "434  E1511  522   773  0.000000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "class DirectedGraph:\n",
    "    def __init__(self, dataset):\n",
    "        self.__feature = dict()\n",
    "        train_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/{}/train.csv\".format(dataset))\n",
    "        test_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/{}/test.csv\".format(dataset))\n",
    "        content_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/{}/content.csv\".format(dataset), header=None)\n",
    "        edge_csv = train_csv[train_csv[\"label\"]==1]\n",
    "        \n",
    "        # get feature\n",
    "        for line in content_csv.to_numpy():\n",
    "            l = line[0].split(\"\\t\")\n",
    "            self.__feature[int(l[0])] = np.array(list(map(int,l[1:])))\n",
    "            \n",
    "        # Create an directed graph G\n",
    "        # from --> to\n",
    "        a_list, b_list, Gtext = [], [], []\n",
    "        for line in edge_csv.to_numpy():\n",
    "            a_list.append(line[2])\n",
    "            b_list.append(line[1])\n",
    "            Gtext.append((line[2], line[1]))\n",
    "            Gtext.append((line[1], line[2]))\n",
    "        self.__edges = [a_list+b_list, b_list+a_list]\n",
    "            \n",
    "        self.__Graph = nx.DiGraph(Gtext)\n",
    "        self.set_feature_nodes()\n",
    "        self.set_feature_weight()\n",
    "        self.set_edges_weight()\n",
    "        \n",
    "        # Random walk with restart\n",
    "        # train_csv['Rwr'] = self.get_rwr_scores(train_csv)\n",
    "        test_csv[\"Rwr\"] =self.get_rwr_scores(test_csv)\n",
    "        # print(\"training dataframe: \")\n",
    "        # print(train_csv.sort_values(by=\"Rwr\", ascending=False).head())\n",
    "        print(\"testing dataframe: \")\n",
    "        print(test_csv.sort_values(by=\"Rwr\", ascending=False).head())\n",
    "            \n",
    "    def get_feature(self):\n",
    "        return self.__feature\n",
    "    \n",
    "    def get_edges(self):\n",
    "        return self.__edges\n",
    "    \n",
    "    def set_feature_nodes(self):\n",
    "        for k in self.__feature.keys():\n",
    "            attributes = np.nonzero(self.__feature[k])\n",
    "            for att in attributes[0]:\n",
    "                self.__Graph.add_edge(f\"a{att}\", k)\n",
    "    \n",
    "    def set_feature_weight(self):\n",
    "        for node in self.__Graph.nodes:\n",
    "            if 'a' in str(node):\n",
    "                att_node_nei = [i for i in self.__Graph.neighbors(node)]\n",
    "                for n in att_node_nei:\n",
    "                    self.__Graph[node][n]['weight'] = 1/len(att_node_nei)\n",
    "                    \n",
    "    def set_edges_weight(self, lam=0.2):\n",
    "        for node in self.__edges[0]:\n",
    "            u_list, a_list = [], []\n",
    "            for n in self.__Graph.neighbors(node):\n",
    "                if 'a' in str(n):\n",
    "                    a_list.append(n)\n",
    "                else:\n",
    "                    u_list.append(n)\n",
    "            if len(u_list)!=0:\n",
    "                for u in u_list:\n",
    "                    self.__Graph[node][u]['weight'] = (1-lam)/len(u_list)\n",
    "            if len(a_list)!=0:\n",
    "                for a in a_list:\n",
    "                    self.__Graph[node][a]['weight'] = lam/len(a_list)\n",
    "    \n",
    "    def get_rwr_scores(self, df):\n",
    "        edges_list = copy(self.__edges)\n",
    "        att_nodes = [node for node in self.__Graph.nodes if 'a' in str(node)]\n",
    "        int_nodes = [node for node in self.__Graph.nodes if 'a' not in str(node)]\n",
    "        \n",
    "        data = [self.__Graph[edges_list[0][i]][edges_list[1][i]]['weight'] for i in range(len(edges_list[0]))]\n",
    "        for i, att in enumerate(att_nodes):\n",
    "            for n in self.__Graph.neighbors(att):\n",
    "                edges_list[0].append(max(int_nodes)+i+1)\n",
    "                edges_list[1].append(n)\n",
    "                data.append(self.__Graph[att][n]['weight'])\n",
    "        \n",
    "        max_size, start_node = max(int_nodes) + 1, min(int_nodes)\n",
    "        matrix_size = max(int_nodes)+len(att_nodes)+1\n",
    "        rwr_score = np.zeros((len(df), 1))\n",
    "        for i in tqdm(range(start_node, max_size), desc=\"rwr scores..\"):\n",
    "            t = df[df['from']==i]\n",
    "            if t.empty is False:\n",
    "                df_idx = t.index\n",
    "                i_scores = rwr(np.array(edges_list), i, matrix_size, data=data, max_iter = 100) \n",
    "                for idx, j in enumerate(t.to.values):\n",
    "                    rwr_score[idx] = i_scores[j]\n",
    "        \n",
    "        return rwr_score\n",
    "\n",
    "        \n",
    "dataset = DirectedGraph(\"dataset3\")\n",
    "feature_node = dataset.get_feature()\n",
    "\n",
    "\n",
    "# G = nx.Graph([(1, 2), (1, 3), (2, 1)])\n",
    "# G[1][2]['weight'] = 0.5\n",
    "# G[2][1]['weight'] = 0.3\n",
    "# print(G[1][2]['weight'])\n",
    "# print(G[2][1]['weight'])\n",
    "# nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fe804b44-1202-4199-87c3-d9d92e2ca60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59 612]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       ...,\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_list = dataset.get_edges()\n",
    "edges_array = np.array(edges_list)\n",
    "print(edges_array[:, 0])\n",
    "r = rwr(edges_array, 1144, 4140)\n",
    "np.sort(r, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a75a29-777b-4202-8df6-e2e33724086c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Randomwalk with restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6f6623df-7c78-4f88-af08-524bc3197d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def rwr(edges, start, matrix_size, alpha = 0.1, epsilon=1e-6, max_iter = 100, data=None):\n",
    "    rows, cols = edges[0,:], edges[1,:]\n",
    "    if data is None:\n",
    "        data = np.ones(edges.shape[1])\n",
    "    adj_mat = csr_matrix((data, (rows, cols)),shape=(matrix_size, matrix_size))\n",
    "    adj_mat_norm = normalize(adj_mat, norm='l1', axis=0)\n",
    "    # create starting vector\n",
    "    start_vec = np.zeros((matrix_size, 1))\n",
    "    start_vec[start] = 1\n",
    "    # init score vector\n",
    "    # old_vec = np.full((matrix_size, 1), 0.8)\n",
    "    old_vec = start_vec\n",
    "    \n",
    "    residuals = np.zeros(max_iter)\n",
    "    for i in range(max_iter):\n",
    "        score_vec = alpha*(adj_mat_norm.dot(old_vec)) + (1-alpha)*start_vec\n",
    "        residuals[i] = norm(score_vec - old_vec, 1) \n",
    "        if residuals[i] < epsilon:\n",
    "            break\n",
    "        old_vec = score_vec\n",
    "        \n",
    "    return score_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d73d1-d36f-4f33-aa93-875474db279f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c6a6e12d-1a28-4a49-b9da-b820c84180ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch.nn import Linear, Sequential, Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9efb9f1d-9f1b-4797-b6a2-0f7b2ae8db07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 6789, 32]             160\n",
      "         LeakyReLU-2             [-1, 6789, 32]               0\n",
      "            Linear-3             [-1, 6789, 64]           2,112\n",
      "         LeakyReLU-4             [-1, 6789, 64]               0\n",
      "            Linear-5            [-1, 6789, 128]           8,320\n",
      "         LeakyReLU-6            [-1, 6789, 128]               0\n",
      "            Linear-7             [-1, 6789, 64]           8,256\n",
      "         LeakyReLU-8             [-1, 6789, 64]               0\n",
      "            Linear-9             [-1, 6789, 32]           2,080\n",
      "        LeakyReLU-10             [-1, 6789, 32]               0\n",
      "           Linear-11             [-1, 6789, 16]             528\n",
      "        LeakyReLU-12             [-1, 6789, 16]               0\n",
      "           Linear-13              [-1, 6789, 1]              17\n",
      "================================================================\n",
      "Total params: 21,473\n",
      "Trainable params: 21,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 34.86\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 35.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, feature):\n",
    "        super(NN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.nn = Sequential(\n",
    "            Linear(feature, 32),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            Linear(32, 64), \n",
    "            torch.nn.LeakyReLU(),\n",
    "            Linear(64, 128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            Linear(128, 64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            Linear(64, 32),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            Linear(32, 16),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.nn(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "training_array, target = data.get_training_data()\n",
    "training_array = torch.tensor(training_array).float()\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_model = NN(X_train.shape[1]).to(\"cpu\")\n",
    "summary(nn_model, input_size = X_train.shape, device=\"cpu\")\n",
    "#print(nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d9d5047a-14f9-4edc-abf7-c36d7180d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(X_train, X_val, y_train, y_val):\n",
    "    # setting\n",
    "    lr = wandb.config[\"learning_rate\"]\n",
    "    MAX_ITERATION = wandb.config[\"MAX_ITERATION\"]\n",
    "    \n",
    "    model = NN(X_train.shape[1])\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')  # Define loss criterion.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(MAX_ITERATION):\n",
    "        optimizer.zero_grad() # clear existing gradients\n",
    "        # compute BC ranking score\n",
    "        out= model(X_train)\n",
    "        # compute loss\n",
    "        #yij, bij = sample_node(y_hat, edges)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, y_train.reshape(-1, 1), reduction=\"sum\")\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"[{}/{}] Loss:{:.4f}\".format(epoch, MAX_ITERATION, loss.item()))\n",
    "        loss.backward()\n",
    "        # validation\n",
    "        val_loss = validation(model, X_val, y_val)\n",
    "        \n",
    "        wandb.log({\"training_loss\": loss}, step=epoch)\n",
    "        wandb.log({\"val_loss\": val_loss}, step=epoch)\n",
    "        optimizer.step()\n",
    "    \n",
    "    #evaluation \n",
    "    y_train_hat = model(X_train)\n",
    "    y_val_hat = model(X_val)\n",
    "    training_auc, training_pre, val_auc, val_pre = evaluation(y_train.detach().numpy(),\n",
    "                                                              y_val.detach().numpy(),\n",
    "                                                              y_train_hat.detach().numpy(),\n",
    "                                                              y_val_hat.detach().numpy())\n",
    "    wandb.summary[\"Training AUC\"] = training_auc\n",
    "    wandb.summary[\"Training PRE\"] = training_pre\n",
    "    wandb.summary[\"Validation AUC\"] = val_auc\n",
    "    wandb.summary[\"Validation PRE\"] = val_pre\n",
    "    return model\n",
    "        \n",
    "def validation(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(X_val)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, y_val.reshape(-1, 1), reduction=\"sum\")\n",
    "    \n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c3b19-e563-47e1-867f-09d897cf1717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc992de-2da0-42d0-9eb7-056e4bae47a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1vxu5hie) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>3424.33008</td></tr><tr><td>val_loss</td><td>399.47021</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dashing-donkey-985</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/1vxu5hie\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/1vxu5hie</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220401_143621-1vxu5hie/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1vxu5hie). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/link_prediction/wandb/run-20220401_143923-d6rqvh65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/d6rqvh65\" target=\"_blank\">northern-frog-986</a></strong> to <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2708/2708 [00:10<00:00, 256.06it/s]\n",
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2708/2708 [00:06<00:00, 440.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing dataframe: \n",
      "          id    to  from     node_pair  ComNei  PreAtt  Jaccard     AA    Rwr\n",
      "2      E3964  2405  1765  (2405, 1765)       0       6    0.000  0.000  0.026\n",
      "1      E4849    81  1634    (81, 1634)       0       6    0.000  0.000  0.011\n",
      "1451   E5663  2104   593   (2104, 593)       0       0    0.000  0.000  0.000\n",
      "1445  E10013  2033    63    (2033, 63)       1       8    0.125  0.417  0.000\n",
      "1446   E3714  1130  2283  (1130, 2283)       0       6    0.000  0.000  0.000\n",
      "[0/10000] Loss:5427.9141\n",
      "[500/10000] Loss:3602.1384\n",
      "[1000/10000] Loss:3541.0305\n",
      "[1500/10000] Loss:3528.0815\n",
      "[2000/10000] Loss:3523.0469\n",
      "[2500/10000] Loss:3519.6436\n",
      "[3000/10000] Loss:3515.8635\n",
      "[3500/10000] Loss:3512.4138\n",
      "[4000/10000] Loss:3507.6292\n",
      "[4500/10000] Loss:3501.8550\n",
      "[5000/10000] Loss:3495.8770\n",
      "[5500/10000] Loss:3485.3613\n",
      "[6000/10000] Loss:3477.8274\n",
      "[6500/10000] Loss:3470.7473\n",
      "[7000/10000] Loss:3465.3237\n",
      "[7500/10000] Loss:3460.7217\n",
      "[8000/10000] Loss:3456.7749\n",
      "[8500/10000] Loss:3455.3547\n",
      "[9000/10000] Loss:3451.8169\n",
      "[9500/10000] Loss:3452.1294\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUC</td><td>0.85316</td></tr><tr><td>Training PRE</td><td>0.87661</td></tr><tr><td>Validation AUC</td><td>0.85793</td></tr><tr><td>Validation PRE</td><td>0.88365</td></tr><tr><td>training_loss</td><td>3448.59326</td></tr><tr><td>val_loss</td><td>403.97058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">northern-frog-986</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/d6rqvh65\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/d6rqvh65</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220401_143923-d6rqvh65/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"NN\")\n",
    "wandb.config[\"learning_rate\"] = 0.0001\n",
    "wandb.config[\"MAX_ITERATION\"] = 10000\n",
    "wandb.config[\"Dataset\"] = \"dataset1\"\n",
    "\n",
    "data = ReadData(wandb.config[\"Dataset\"])\n",
    "training_array, target = data.get_training_data()\n",
    "training_array = torch.tensor(training_array).float()\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.1)\n",
    "nn_model1 = train(X_train, X_val, y_train, y_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b80b4c13-0420-4fa7-b217-21f984fe494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/link_prediction/wandb/run-20220401_144213-1h8sxjss</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/1h8sxjss\" target=\"_blank\">pious-salad-987</a></strong> to <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 3311/3311 [00:13<00:00, 245.08it/s]\n",
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 3311/3311 [00:06<00:00, 516.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing dataframe: \n",
      "         id    to  from     node_pair  ComNei  PreAtt  Jaccard     AA    Rwr\n",
      "3     E5670  1063  1101  (1063, 1101)       2     468    0.024  1.135  0.024\n",
      "4     E5005  1067  1710  (1067, 1710)       0      14    0.000  0.000  0.009\n",
      "0     E3064  1315   586   (1315, 586)       0      14    0.000  0.000  0.000\n",
      "1254  E8203  1336    35    (1336, 35)       0       2    0.000  0.000  0.000\n",
      "1266  E6154   661   485    (661, 485)       0       3    0.000  0.000  0.000\n",
      "[0/10000] Loss:4714.1211\n",
      "[500/10000] Loss:3325.3040\n",
      "[1000/10000] Loss:3271.2559\n",
      "[1500/10000] Loss:3211.3828\n",
      "[2000/10000] Loss:3165.9941\n",
      "[2500/10000] Loss:3153.4055\n",
      "[3000/10000] Loss:3146.3865\n",
      "[3500/10000] Loss:3142.2180\n",
      "[4000/10000] Loss:3139.3093\n",
      "[4500/10000] Loss:3137.2544\n",
      "[5000/10000] Loss:3133.7568\n",
      "[5500/10000] Loss:3129.4590\n",
      "[6000/10000] Loss:3124.4360\n",
      "[6500/10000] Loss:3119.7920\n",
      "[7000/10000] Loss:3114.2019\n",
      "[7500/10000] Loss:3106.8125\n",
      "[8000/10000] Loss:3099.3953\n",
      "[8500/10000] Loss:3093.5342\n",
      "[9000/10000] Loss:3090.3906\n",
      "[9500/10000] Loss:3088.3914\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUC</td><td>0.83158</td></tr><tr><td>Training PRE</td><td>0.85155</td></tr><tr><td>Validation AUC</td><td>0.84855</td></tr><tr><td>Validation PRE</td><td>0.86695</td></tr><tr><td>training_loss</td><td>3087.03027</td></tr><tr><td>val_loss</td><td>341.44824</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pious-salad-987</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/1h8sxjss\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/1h8sxjss</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220401_144213-1h8sxjss/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"NN\")\n",
    "wandb.config[\"learning_rate\"] = 0.0001\n",
    "wandb.config[\"MAX_ITERATION\"] = 10000\n",
    "wandb.config[\"Dataset\"] = \"dataset2\"\n",
    "\n",
    "data = ReadData(wandb.config[\"Dataset\"])\n",
    "training_array, target = data.get_training_data()\n",
    "training_array = torch.tensor(training_array).float()\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.1)\n",
    "nn_model2 = train(X_train, X_val, y_train, y_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19d99e15-13ea-4eff-94ca-9041118d094a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/link_prediction/wandb/run-20220401_144506-366vi8x2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/366vi8x2\" target=\"_blank\">wise-snowball-988</a></strong> to <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 876/876 [00:01<00:00, 520.30it/s]\n",
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 876/876 [00:00<00:00, 894.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing dataframe: \n",
      "        id   to  from   node_pair  ComNei  PreAtt  Jaccard     AA    Rwr\n",
      "5    E1391  117   793  (117, 793)       1       4    0.333  0.721  0.016\n",
      "4    E2161  466   199  (466, 199)       0       1    0.000  0.000  0.008\n",
      "2    E3190  739   468  (739, 468)       0       8    0.000  0.000  0.001\n",
      "0     E370   26   317   (26, 317)       0       6    0.000  0.000  0.000\n",
      "434  E1511  522   773  (522, 773)       1       2    0.500  0.721  0.000\n",
      "[0/10000] Loss:1607.4530\n",
      "[500/10000] Loss:957.5345\n",
      "[1000/10000] Loss:923.1216\n",
      "[1500/10000] Loss:906.2383\n",
      "[2000/10000] Loss:897.7651\n",
      "[2500/10000] Loss:888.2858\n",
      "[3000/10000] Loss:882.5832\n",
      "[3500/10000] Loss:879.2104\n",
      "[4000/10000] Loss:875.0524\n",
      "[4500/10000] Loss:870.3646\n",
      "[5000/10000] Loss:865.9105\n",
      "[5500/10000] Loss:862.6511\n",
      "[6000/10000] Loss:860.5424\n",
      "[6500/10000] Loss:857.6019\n",
      "[7000/10000] Loss:854.8506\n",
      "[7500/10000] Loss:853.2886\n",
      "[8000/10000] Loss:852.1823\n",
      "[8500/10000] Loss:851.5554\n",
      "[9000/10000] Loss:850.5000\n",
      "[9500/10000] Loss:850.5607\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUC</td><td>0.90124</td></tr><tr><td>Training PRE</td><td>0.91267</td></tr><tr><td>Validation AUC</td><td>0.88124</td></tr><tr><td>Validation PRE</td><td>0.89188</td></tr><tr><td>training_loss</td><td>849.4801</td></tr><tr><td>val_loss</td><td>148.82861</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wise-snowball-988</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/366vi8x2\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/366vi8x2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220401_144506-366vi8x2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"NN\")\n",
    "wandb.config[\"learning_rate\"] = 0.0001\n",
    "wandb.config[\"MAX_ITERATION\"] = 10000\n",
    "wandb.config[\"Dataset\"] = \"dataset3\"\n",
    "\n",
    "data = ReadData(wandb.config[\"Dataset\"])\n",
    "training_array, target = data.get_training_data()\n",
    "training_array = torch.tensor(training_array).float()\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.1)\n",
    "nn_model3 = train(X_train, X_val, y_train, y_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3e6e9-6891-4119-8682-f72d6e5dc817",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78aaf482-223c-417f-bbc3-35f11bba9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "def RandomForest(dataset, config=False):\n",
    "    training_array, target = dataset.get_training_data()\n",
    "    reg = RandomForestRegressor(n_estimators=int(config[\"n_estimators\"]),\n",
    "                               min_samples_leaf=int(config[\"min_samples_leaf\"]),\n",
    "                               min_samples_split=int(config[\"min_samples_split\"]),\n",
    "                               oob_score=bool(config[\"oob_score\"]))\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.2)\n",
    "    y_train = y_train.astype('int')\n",
    "    y_val = y_val.astype('int')\n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_train_hat = np.where(reg.predict(X_train)>0.5, 1, 0)\n",
    "    y_val_hat = np.where(reg.predict(X_val)>0.5, 1, 0)\n",
    "\n",
    "    training_auc, training_pre, val_auc, val_pre = evaluation(y_train, y_val, y_train_hat, y_val_hat)\n",
    "    wandb.summary[\"feature_number\"] = X_train.shape[1]\n",
    "    wandb.log({\n",
    "        \"Training AUC\":training_auc,\n",
    "        \"Training PRE\": training_pre,\n",
    "        \"Validation AUC\": val_auc,\n",
    "        \"Validation PRE\": val_pre\n",
    "    })\n",
    "\n",
    "    joblib.dump(reg, os.path.join(wandb.run.dir, 'rf_model.h5'))\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb3187-9ed3-4828-806e-5738c69f6c6b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9e2391-10f1-4d28-b994-95804804fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "from IPython.display import clear_output\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\":{\n",
    "        \"name\": \"Validation PRE\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \n",
    "    \"parameters\":{\n",
    "        \"n_estimators\":{\n",
    "            'min': 10,\n",
    "            'max': 150\n",
    "        },\n",
    "        \"min_samples_leaf\":{\n",
    "            'min': 1,\n",
    "            'max': 50,\n",
    "        },\n",
    "        \"min_samples_split\":{\n",
    "            'min': 2,\n",
    "            'max': 5\n",
    "        },\n",
    "        \"oob_score\":{\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56a0f868-46de-485a-9197-ce6e7a8cb041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_dataset1_rf():\n",
    "    with wandb.init(group=\"rf\") as run:\n",
    "        config = wandb.config\n",
    "        model = RandomForest(dataset1, config)\n",
    "        \n",
    "def train_dataset2_rf():\n",
    "    with wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"rf\") as run:\n",
    "        config = wandb.config\n",
    "        model = RandomForest(dataset2, config)\n",
    "\n",
    "def train_dataset3_rf():\n",
    "    with wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"rf\") as run:\n",
    "        config = wandb.config\n",
    "        model = RandomForest(dataset3, config)\n",
    "\n",
    "        \n",
    "count = 100 # number of runs to execute\n",
    "# training dataset 1\n",
    "dataset1 = ReadData(\"dataset1\")\n",
    "sweep_id = wandb.sweep(sweep_config, project='Link_Prediction@MLG', entity=\"baron\")\n",
    "wandb.agent(sweep_id, function=train_dataset1_rf, count=count)\n",
    "clear_output()\n",
    "# training dataset 2\n",
    "dataset2 = ReadData(\"dataset2\")\n",
    "sweep_id = wandb.sweep(sweep_config, project='Link_Prediction@MLG', entity=\"baron\")\n",
    "wandb.agent(sweep_id, function=train_dataset2_rf, count=count)\n",
    "clear_output()\n",
    "# training dataset 3\n",
    "dataset3 = ReadData(\"dataset3\")\n",
    "sweep_id = wandb.sweep(sweep_config, project='Link_Prediction@MLG', entity=\"baron\")\n",
    "wandb.agent(sweep_id, function=train_dataset3_rf, count=count)  \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e852b74f-26b3-4533-aa73-b5ef4c8aeb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1: \n",
      "training auc: 0.779 pre: 0.732, validation auc: 0.778 pre: 0.743\n",
      "Dataset2: \n",
      "training auc: 0.747 pre: 0.702, validation auc: 0.751 pre: 0.710\n",
      "Dataset3: \n",
      "training auc: 0.816 pre: 0.781, validation auc: 0.821 pre: 0.764\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "def dict_to_config(d):\n",
    "    class Object(object):\n",
    "        pass\n",
    "\n",
    "    config = Object()\n",
    "    for key, value in d.items():\n",
    "        setattr(config, key, value)\n",
    "    return config\n",
    "\n",
    "def parse_wandb_models(path, model_name, numbers_models=None, metric=None):\n",
    "    '''Parse wandb models with either run paths or a sweep path.\n",
    "\n",
    "    Args:\n",
    "        path: a list contains either run paths or a sweep path\n",
    "        numbers_models: a integer or a list of numbers of models.\n",
    "                        if None, treat path as run paths, otherwise treat it as a sweep path.\n",
    "        metric: metric to sort by when parsing a sweep path\n",
    "    '''\n",
    "    api = wandb.Api()\n",
    "    models, configs, model_paths = list(), list(), list()\n",
    "    sweep_name = ''\n",
    "\n",
    "    modeldir = tempfile.mkdtemp()\n",
    "\n",
    "    if numbers_models is not None: # sweep\n",
    "        numbers_models = max(numbers_models) if isinstance(numbers_models, list) else numbers_models\n",
    "\n",
    "        sweep = api.sweep(path[0])\n",
    "        sweep_name = sweep.config.get('name', '')\n",
    "        # sort runs by metric\n",
    "        runs = sorted(sweep.runs, key=lambda run: run.summary.get(metric, 0), \n",
    "                            reverse=True)\n",
    "        runs = runs[:numbers_models]\n",
    "        \n",
    "    else:\n",
    "        runs = [api.run(p) for p in path]\n",
    "\n",
    "    for run in runs:\n",
    "        run.file('{}.h5'.format(model_name)).download(replace=True, root=modeldir)\n",
    "\n",
    "        # load_model =\n",
    "        models.append(joblib.load(modeldir + '/{}.h5'.format(model_name)))\n",
    "\n",
    "        configs.append(dict_to_config(run.config))\n",
    "        model_paths.append(run.path)\n",
    "\n",
    "    return models, configs, model_paths, sweep_name\n",
    "\n",
    "def MeanAccuracy(dataset, models):\n",
    "    sum_training_auc, sum_training_pre, sum_val_auc, sum_val_pre = 0, 0, 0, 0\n",
    "    n = len(model_paths)\n",
    "    training_array, target = dataset.get_training_data()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.2)\n",
    "    for model in models:\n",
    "        y_train_hat = np.where(model.predict(X_train)>0.5, 1, 0)\n",
    "        y_val_hat = np.where(model.predict(X_val)>0.5, 1, 0)\n",
    "        y_train = y_train.astype('int')\n",
    "        y_val = y_val.astype('int')\n",
    "        training_auc, training_pre, val_auc, val_pre = evaluation(y_train, y_val, y_train_hat, y_val_hat)\n",
    "        sum_training_auc += training_auc\n",
    "        sum_training_pre += training_pre\n",
    "        sum_val_auc +=  val_auc\n",
    "        sum_val_pre +=  val_pre\n",
    "    print(\"training auc: {:.3f} pre: {:.3f}, validation auc: {:.3f} pre: {:.3f}\".format(sum_training_auc/n, sum_training_pre/n, sum_val_auc/n, sum_val_pre/n))\n",
    "\n",
    "\n",
    "print(\"Dataset1: \")\n",
    "rf_models1, configs, model_paths, sweep_name = parse_wandb_models(path=[\"baron/Link_Prediction@MLG/4uf9wyjf\"], model_name=\"rf_model\", numbers_models=5, metric=\"Validation PRE\")\n",
    "MeanAccuracy(dataset1, rf_models1)\n",
    "print(\"Dataset2: \")\n",
    "rf_models2, configs, model_paths, sweep_name = parse_wandb_models(path=[\"baron/Link_Prediction@MLG/z8qpl1kf\"], model_name=\"rf_model\", numbers_models=5, metric=\"Validation PRE\")\n",
    "MeanAccuracy(dataset2, rf_models2)\n",
    "print(\"Dataset3: \")\n",
    "rf_models3, configs, model_paths, sweep_name = parse_wandb_models(path=[\"baron/Link_Prediction@MLG/r8n1nt9r\"], model_name=\"rf_model\", numbers_models=5, metric=\"Validation PRE\")\n",
    "MeanAccuracy(dataset3, rf_models3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96384b74-5986-4122-83f5-a7526ff9ede1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. GCNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6f84a0-75fa-4377-844d-d31e9cf03c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8d43b9-c721-4bb9-85ec-06ea04ff4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GAE\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_chaneels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.input = GCNConv(in_channels, 512, cached=True)\n",
    "        self.output = GCNConv(512, out_chaneels, cached=True)\n",
    "        \n",
    "    # def forward(self, x, edge_index):\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.input(x, edge_index).relu()\n",
    "        return self.output(x, edge_index)\n",
    "    \n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "    \n",
    "    def predict(self, z, node_pairs):\n",
    "        logits = (z[node_pairs[0]] * z[node_pairs[1]]).sum(dim=-1)  # dot product \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5077556-a3fb-4737-b687-7d9f3f45fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index, device = torch.device('cpu')):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equel to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "def train(pg_data, device = torch.device('cpu')):\n",
    "    # training data\n",
    "    x = pg_data.x.to(device)\n",
    "    train_pos_edge_index = pg_data.train_pos_edge_index.to(device)\n",
    "    \n",
    "    # parameters\n",
    "    out_channel = 128\n",
    "    epochs = 200\n",
    "    lr = 0.0001\n",
    "    num_feature = x.shape[1]\n",
    "    \n",
    "    # init Model \n",
    "    model = GCNEncoder(num_feature, out_channel)\n",
    "    \n",
    "    # init the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # generate negative edge\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_pos_edge_index, #positive edges\n",
    "            num_nodes=pg_data.num_nodes, # number of nodes\n",
    "            num_neg_samples=train_pos_edge_index.shape[1]) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "        # encoder_model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "        link_logits = model.decode(z, train_pos_edge_index, neg_edge_index) # decode\n",
    "        link_labels = get_link_labels(train_pos_edge_index, neg_edge_index)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        auc, ap = test(model, x)\n",
    "        if epoch%100 ==0:\n",
    "            print(\"[{}/{}] Val AUC: {:.4f}, Test AUC: {:.4f},  Val AP: {:.4f}, Test AP: {:.4f}\".format(epoch, epochs, auc[0], ap[0], auc[1], ap[1]))\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        perfs = []\n",
    "        perfs_pr = []\n",
    "        for prefix in [\"val\", \"test\"]:\n",
    "            pos_edge_index = pg_data[f'{prefix}_pos_edge_index']\n",
    "            neg_edge_index = pg_data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "            z = model.encode(x, pos_edge_index) # encode train\n",
    "            link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "            link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "\n",
    "            link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "\n",
    "            perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "            perfs_pr.append(average_precision_score(link_labels.cpu(), link_probs.cpu()))\n",
    "        return perfs, perfs_pr\n",
    "    # return encoder_model.test(z, pos_edge_index, neg_edge_index)\n",
    "\n",
    "# for epoch in range(1, epochs+1):\n",
    "#     loss = train(x, train_pos_edge_index)\n",
    "#     auc, pre = test(encoder_model, x)\n",
    "#     # auc, ap = test(pg_data.test_pos_edge_index, pg_data.test_neg_edge_index)\n",
    "#     if epoch%50 ==0:\n",
    "#         # print(\"Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}\".format(epoch, auc, ap))\n",
    "#         print(\"[{}/{}] Val AUC: {:.4f}, Test AUC: {:.4f},  Val PRE: {:.4f}, Test PRE: {:.4f}\".format(epoch, epochs, auc[0], pre[0], auc[1], pre[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315f43e-bb0d-4908-b52d-bc3278c514b1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc343ebc-85e2-42f1-a922-559403a06392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To training dataset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/200] Val AUC: 0.9231, Test AUC: 0.9270,  Val AP: 0.9302, Test AP: 0.9275\n",
      "[200/200] Val AUC: 0.9389, Test AUC: 0.9430,  Val AP: 0.9400, Test AP: 0.9434\n",
      "To training dataset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/200] Val AUC: 0.9434, Test AUC: 0.9527,  Val AP: 0.9498, Test AP: 0.9537\n",
      "[200/200] Val AUC: 0.9321, Test AUC: 0.9443,  Val AP: 0.9471, Test AP: 0.9540\n",
      "To training dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/200] Val AUC: 0.9503, Test AUC: 0.9515,  Val AP: 0.9516, Test AP: 0.9356\n",
      "[200/200] Val AUC: 0.9258, Test AUC: 0.9391,  Val AP: 0.9506, Test AP: 0.9369\n"
     ]
    }
   ],
   "source": [
    "print(\"To training dataset 1\")\n",
    "data = ReadData(\"dataset1\")\n",
    "edge_index = data.get_torch_edges()\n",
    "x = data.get_nodes_x()\n",
    "pg_data = Data(x=x, edge_index=edge_index)\n",
    "pg_data = train_test_split_edges(pg_data)\n",
    "# model\n",
    "gcn_model1 = train(pg_data)\n",
    "\n",
    "print(\"To training dataset 2\")\n",
    "data = ReadData(\"dataset2\")\n",
    "edge_index = data.get_torch_edges()\n",
    "x = data.get_nodes_x()\n",
    "pg_data = Data(x=x, edge_index=edge_index)\n",
    "pg_data = train_test_split_edges(pg_data)\n",
    "# model\n",
    "gcn_model2 = train(pg_data)\n",
    "\n",
    "print(\"To training dataset 3\")\n",
    "data = ReadData(\"dataset3\")\n",
    "edge_index = data.get_torch_edges()\n",
    "x = data.get_nodes_x()\n",
    "pg_data = Data(x=x, edge_index=edge_index)\n",
    "pg_data = train_test_split_edges(pg_data)\n",
    "# model\n",
    "gcn_model3 = train(pg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14c4e5-2e69-4602-8e9d-ca65cbd1c616",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.GCN + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50b42f43-77ff-459a-8b98-4c68da18e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "def RandomForest(dataset, config=False, link_probs=False):\n",
    "    training_array, target = dataset.get_training_data()\n",
    "    if config:\n",
    "        reg = RandomForestRegressor(n_estimators=int(config[\"n_estimators\"]),\n",
    "                                       min_samples_leaf=int(config[\"min_samples_leaf\"]),\n",
    "                                       min_samples_split=int(config[\"min_samples_split\"]),\n",
    "                                       oob_score=bool(config[\"oob_score\"]))\n",
    "    else:\n",
    "        reg = RandomForestRegressor()\n",
    "        \n",
    "    if link_probs.shape:\n",
    "        training_array = np.concatenate((training_array, link_probs.cpu().detach().numpy().reshape(-1, 1)), axis=1)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.2)\n",
    "    y_train = y_train.astype('int')\n",
    "    y_val = y_val.astype('int')\n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_train_hat = np.where(reg.predict(X_train)>0.5, 1, 0)\n",
    "    y_val_hat = np.where(reg.predict(X_val)>0.5, 1, 0)\n",
    "\n",
    "    training_auc, training_pre, val_auc, val_pre = evaluation(y_train, y_val, y_train_hat, y_val_hat)\n",
    "\n",
    "    wandb.summary[\"feature_number\"] = X_train.shape[1]\n",
    "    wandb.log({\n",
    "        \"Training AUC\":training_auc,\n",
    "        \"Training PRE\": training_pre,\n",
    "        \"Validation AUC\": val_auc,\n",
    "        \"Validation PRE\": val_pre\n",
    "    })\n",
    "\n",
    "    joblib.dump(reg, os.path.join(wandb.run.dir, 'rf_model.h5'))\n",
    "    \n",
    "    return reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e5841-9006-4595-84e1-ba7a67c1ebab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f051ae09-883a-4ea9-90ab-2c009a2b703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "from IPython.display import clear_output\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\":{\n",
    "        \"name\": \"Validation PRE\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \n",
    "    \"parameters\":{\n",
    "        \"n_estimators\":{\n",
    "            'min': 10,\n",
    "            'max': 150\n",
    "        },\n",
    "        \"min_samples_leaf\":{\n",
    "            'min': 1,\n",
    "            'max': 50,\n",
    "        },\n",
    "        \"min_samples_split\":{\n",
    "            'min': 2,\n",
    "            'max': 5\n",
    "        },\n",
    "        \"oob_score\":{\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fe27541-b774-4fa4-b699-7ceb2bb4b2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_dataset1_rf():\n",
    "    with wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"rf\") as run:\n",
    "        config = wandb.config\n",
    "        model = RandomForest(dataset1, config, link_probs=link_probs)\n",
    "        \n",
    "def train_dataset2_rf():\n",
    "    with wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"rf\") as run:\n",
    "        config = wandb.config\n",
    "        model = RandomForest(dataset2, config, link_probs=link_probs)\n",
    "\n",
    "def train_dataset3_rf():\n",
    "    with wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"rf\") as run:\n",
    "        config = wandb.config\n",
    "        model = RandomForest(dataset3, config, link_probs=link_probs)\n",
    "\n",
    "        \n",
    "count = 100 # number of runs to execute\n",
    "# training dataset 1\n",
    "dataset1 = ReadData(\"dataset1\")\n",
    "x = dataset1.get_nodes_x()\n",
    "node_pairs = dataset1.get_training_nodepairs()\n",
    "link_logits = gcn_model1.predict(x, node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='Link_Prediction@MLG', entity=\"baron\")\n",
    "wandb.agent(sweep_id, function=train_dataset1_rf, count=count)\n",
    "clear_output()\n",
    "# training dataset 2\n",
    "dataset2 = ReadData(\"dataset2\")\n",
    "x = dataset2.get_nodes_x()\n",
    "node_pairs = dataset2.get_training_nodepairs()\n",
    "link_logits = gcn_model2.predict(x, node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='Link_Prediction@MLG', entity=\"baron\")\n",
    "wandb.agent(sweep_id, function=train_dataset2_rf, count=count)\n",
    "clear_output()\n",
    "# training dataset 3\n",
    "dataset3 = ReadData(\"dataset3\")\n",
    "x = dataset3.get_nodes_x()\n",
    "node_pairs = dataset3.get_training_nodepairs()\n",
    "link_logits = gcn_model3.predict(x, node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='Link_Prediction@MLG', entity=\"baron\")\n",
    "wandb.agent(sweep_id, function=train_dataset3_rf, count=count)  \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "446e419b-7181-4ec6-96e4-a1c8d935cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1: \n",
      "training auc: 0.848 pre: 0.807, validation auc: 0.858 pre: 0.819\n",
      "Dataset2: \n",
      "training auc: 0.876 pre: 0.837, validation auc: 0.886 pre: 0.852\n",
      "Dataset3: \n",
      "training auc: 0.815 pre: 0.774, validation auc: 0.826 pre: 0.780\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "def dict_to_config(d):\n",
    "    class Object(object):\n",
    "        pass\n",
    "\n",
    "    config = Object()\n",
    "    for key, value in d.items():\n",
    "        setattr(config, key, value)\n",
    "    return config\n",
    "\n",
    "def parse_wandb_models(path, model_name, numbers_models=None, metric=None):\n",
    "    '''Parse wandb models with either run paths or a sweep path.\n",
    "\n",
    "    Args:\n",
    "        path: a list contains either run paths or a sweep path\n",
    "        numbers_models: a integer or a list of numbers of models.\n",
    "                        if None, treat path as run paths, otherwise treat it as a sweep path.\n",
    "        metric: metric to sort by when parsing a sweep path\n",
    "    '''\n",
    "    api = wandb.Api()\n",
    "    models, configs, model_paths = list(), list(), list()\n",
    "    sweep_name = ''\n",
    "\n",
    "    modeldir = tempfile.mkdtemp()\n",
    "\n",
    "    if numbers_models is not None: # sweep\n",
    "        numbers_models = max(numbers_models) if isinstance(numbers_models, list) else numbers_models\n",
    "\n",
    "        sweep = api.sweep(path[0])\n",
    "        sweep_name = sweep.config.get('name', '')\n",
    "        # sort runs by metric\n",
    "        runs = sorted(sweep.runs, key=lambda run: run.summary.get(metric, 0), \n",
    "                            reverse=True)\n",
    "        runs = runs[:numbers_models]\n",
    "        \n",
    "    else:\n",
    "        runs = [api.run(p) for p in path]\n",
    "\n",
    "    for run in runs:\n",
    "        run.file('{}.h5'.format(model_name)).download(replace=True, root=modeldir)\n",
    "\n",
    "        # load_model =\n",
    "        models.append(joblib.load(modeldir + '/{}.h5'.format(model_name)))\n",
    "\n",
    "        configs.append(dict_to_config(run.config))\n",
    "        model_paths.append(run.path)\n",
    "\n",
    "    return models, configs, model_paths, sweep_name\n",
    "\n",
    "def MeanAccuracy(dataset, models, link_probs):\n",
    "    sum_training_auc, sum_training_pre, sum_val_auc, sum_val_pre = 0, 0, 0, 0\n",
    "    n = len(model_paths)\n",
    "    \n",
    "    training_array, target = dataset.get_training_data()\n",
    "    training_array = np.concatenate((training_array, link_probs.cpu().detach().numpy().reshape(-1, 1)), axis=1)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(training_array, target, test_size=0.2)\n",
    "    for model in models:\n",
    "        y_train_hat = np.where(model.predict(X_train)>0.5, 1, 0)\n",
    "        y_val_hat = np.where(model.predict(X_val)>0.5, 1, 0)\n",
    "        y_train = y_train.astype('int')\n",
    "        y_val = y_val.astype('int')\n",
    "        training_auc, training_pre, val_auc, val_pre = evaluation(y_train, y_val, y_train_hat, y_val_hat)\n",
    "        sum_training_auc += training_auc\n",
    "        sum_training_pre += training_pre\n",
    "        sum_val_auc +=  val_auc\n",
    "        sum_val_pre +=  val_pre\n",
    "    print(\"training auc: {:.3f} pre: {:.3f}, validation auc: {:.3f} pre: {:.3f}\".format(sum_training_auc/n, sum_training_pre/n, sum_val_auc/n, sum_val_pre/n))\n",
    "\n",
    "\n",
    "print(\"Dataset1: \")\n",
    "# gcn \n",
    "x = dataset1.get_nodes_x()\n",
    "node_pairs = dataset1.get_training_nodepairs()\n",
    "link_logits = gcn_model1.predict(x, node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "gcn_rf_models1, configs, model_paths, sweep_name = parse_wandb_models(path=[\"baron/Link_Prediction@MLG/p9u07rhq\"], model_name=\"rf_model\", numbers_models=5, metric=\"Validation PRE\")\n",
    "MeanAccuracy(dataset1, gcn_rf_models1, link_probs)\n",
    "\n",
    "print(\"Dataset2: \")\n",
    "# gcn \n",
    "x = dataset2.get_nodes_x()\n",
    "node_pairs = dataset2.get_training_nodepairs()\n",
    "link_logits = gcn_model2.predict(x, node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "gcn_rf_models2, configs, model_paths, sweep_name = parse_wandb_models(path=[\"baron/Link_Prediction@MLG/9k3h2wdu\"], model_name=\"rf_model\", numbers_models=5, metric=\"Validation PRE\")\n",
    "MeanAccuracy(dataset2, gcn_rf_models2, link_probs)\n",
    "\n",
    "print(\"Dataset3: \")\n",
    "# gcn \n",
    "x = dataset3.get_nodes_x()\n",
    "node_pairs = dataset3.get_training_nodepairs()\n",
    "link_logits = gcn_model3.predict(x, node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "gcn_rf_models3, configs, model_paths, sweep_name = parse_wandb_models(path=[\"baron/Link_Prediction@MLG/zcjubt87\"], model_name=\"rf_model\", numbers_models=5, metric=\"Validation PRE\")\n",
    "MeanAccuracy(dataset3, gcn_rf_models3, link_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3c360-4ce8-4aca-943f-d6d4aca6265a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. GCN+NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "2a71db75-d99b-4bae-9d84-1d493e56cfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Ignored wandb.init() arg project when running a sweep.\n",
      "wandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/link_prediction/wandb/run-20220408_233914-ym48o749</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749\" target=\"_blank\">autumn-sweep-100</a></strong> to <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/sweeps/r8n1nt9r\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/sweeps/r8n1nt9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000] Loss:5429.8076\n",
      "[500/10000] Loss:2842.8120\n",
      "[1000/10000] Loss:2757.0232\n",
      "[1500/10000] Loss:2724.2178\n",
      "[2000/10000] Loss:2709.4746\n",
      "[2500/10000] Loss:2699.7729\n",
      "[3000/10000] Loss:2689.2524\n",
      "[3500/10000] Loss:2678.4617\n",
      "[4000/10000] Loss:2668.6633\n",
      "[4500/10000] Loss:2658.2334\n",
      "[5000/10000] Loss:2647.7224\n",
      "[5500/10000] Loss:2638.2883\n",
      "[6000/10000] Loss:2630.0439\n",
      "[6500/10000] Loss:2623.0024\n",
      "[7000/10000] Loss:2616.2830\n",
      "[7500/10000] Loss:2610.8594\n",
      "[8000/10000] Loss:2604.5078\n",
      "[8500/10000] Loss:2598.6121\n",
      "[9000/10000] Loss:2594.2192\n",
      "[9500/10000] Loss:2590.9426\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUC</td><td>0.92548</td></tr><tr><td>Training PRE</td><td>0.93709</td></tr><tr><td>Validation AUC</td><td>0.91825</td></tr><tr><td>Validation PRE</td><td>0.93374</td></tr><tr><td>training_loss</td><td>2588.13379</td></tr><tr><td>val_loss</td><td>411.42435</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-sweep-100</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_233914-ym48o749/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"GCN\")\n",
    "wandb.config[\"learning_rate\"] = 0.0001\n",
    "wandb.config[\"MAX_ITERATION\"] = 10000\n",
    "wandb.config[\"Dataset\"] = \"dataset1\"\n",
    "\n",
    "dataset1 = ReadData(wandb.config[\"Dataset\"])\n",
    "x = dataset1.get_nodes_x()\n",
    "node_pairs = dataset1.get_training_nodepairs()\n",
    "link_probs = gcn_model1.predict(x, node_pairs)\n",
    "# link_probs = link_logits.sigmoid()\n",
    "# neighbor feature\n",
    "training_array, target = dataset1.get_training_data()\n",
    "training_data = torch.cat((torch.tensor(training_array, dtype=torch.float), link_probs.reshape(-1, 1)), 1)\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_data, target, test_size=0.1)\n",
    "gcn_nn_model1 = train(X_train, X_val, y_train, y_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ba3e34ef-79bc-44f7-a232-f23401a964a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Ignored wandb.init() arg project when running a sweep.\n",
      "wandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/link_prediction/wandb/run-20220408_234150-ym48o749</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749\" target=\"_blank\">autumn-sweep-100</a></strong> to <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/sweeps/r8n1nt9r\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/sweeps/r8n1nt9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000] Loss:4719.3384\n",
      "[500/10000] Loss:2065.1960\n",
      "[1000/10000] Loss:1987.9291\n",
      "[1500/10000] Loss:1963.9651\n",
      "[2000/10000] Loss:1947.6254\n",
      "[2500/10000] Loss:1934.4635\n",
      "[3000/10000] Loss:1923.9811\n",
      "[3500/10000] Loss:1913.7996\n",
      "[4000/10000] Loss:1900.3619\n",
      "[4500/10000] Loss:1890.3495\n",
      "[5000/10000] Loss:1883.0386\n",
      "[5500/10000] Loss:1873.4359\n",
      "[6000/10000] Loss:1868.3459\n",
      "[6500/10000] Loss:1863.8324\n",
      "[7000/10000] Loss:1859.8173\n",
      "[7500/10000] Loss:1856.1672\n",
      "[8000/10000] Loss:1853.9807\n",
      "[8500/10000] Loss:1851.3228\n",
      "[9000/10000] Loss:1847.1819\n",
      "[9500/10000] Loss:1845.3076\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUC</td><td>0.95214</td></tr><tr><td>Training PRE</td><td>0.95763</td></tr><tr><td>Validation AUC</td><td>0.94712</td></tr><tr><td>Validation PRE</td><td>0.94995</td></tr><tr><td>training_loss</td><td>1843.21777</td></tr><tr><td>val_loss</td><td>222.42969</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-sweep-100</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_234150-ym48o749/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"GCN\")\n",
    "wandb.config[\"learning_rate\"] = 0.0001\n",
    "wandb.config[\"MAX_ITERATION\"] = 10000\n",
    "wandb.config[\"Dataset\"] = \"dataset2\"\n",
    "\n",
    "dataset2 = ReadData(wandb.config[\"Dataset\"])\n",
    "x = dataset2.get_nodes_x()\n",
    "node_pairs = dataset2.get_training_nodepairs()\n",
    "link_probs = gcn_model2.predict(x, node_pairs)\n",
    "# link_probs = link_logits.sigmoid()\n",
    "# neighbor feature\n",
    "training_array, target = dataset2.get_training_data()\n",
    "training_data = torch.cat((torch.tensor(training_array, dtype=torch.float), link_probs.reshape(-1, 1)), 1)\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_data, target, test_size=0.1)\n",
    "gcn_nn_model2 = train(X_train, X_val, y_train, y_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "80a1c471-d314-49f1-9b6f-d5950f61eadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Ignored wandb.init() arg project when running a sweep.\n",
      "wandb: WARNING Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/baron/HW/link_prediction/wandb/run-20220408_234422-ym48o749</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749\" target=\"_blank\">autumn-sweep-100</a></strong> to <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/sweeps/r8n1nt9r\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/sweeps/r8n1nt9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20000] Loss:1609.2957\n",
      "[500/20000] Loss:940.3589\n",
      "[1000/20000] Loss:907.5302\n",
      "[1500/20000] Loss:894.5874\n",
      "[2000/20000] Loss:883.3215\n",
      "[2500/20000] Loss:868.7643\n",
      "[3000/20000] Loss:860.5479\n",
      "[3500/20000] Loss:857.1821\n",
      "[4000/20000] Loss:853.3588\n",
      "[4500/20000] Loss:851.9291\n",
      "[5000/20000] Loss:848.0945\n",
      "[5500/20000] Loss:845.7708\n",
      "[6000/20000] Loss:842.5715\n",
      "[6500/20000] Loss:839.7527\n",
      "[7000/20000] Loss:836.7341\n",
      "[7500/20000] Loss:834.3440\n",
      "[8000/20000] Loss:832.6584\n",
      "[8500/20000] Loss:831.9530\n",
      "[9000/20000] Loss:831.7958\n",
      "[9500/20000] Loss:828.9706\n",
      "[10000/20000] Loss:827.7080\n",
      "[10500/20000] Loss:827.2259\n",
      "[11000/20000] Loss:825.5567\n",
      "[11500/20000] Loss:824.3710\n",
      "[12000/20000] Loss:824.4882\n",
      "[12500/20000] Loss:822.8684\n",
      "[13000/20000] Loss:818.4312\n",
      "[13500/20000] Loss:816.9916\n",
      "[14000/20000] Loss:815.8712\n",
      "[14500/20000] Loss:814.6362\n",
      "[15000/20000] Loss:814.4839\n",
      "[15500/20000] Loss:813.1956\n",
      "[16000/20000] Loss:814.0078\n",
      "[16500/20000] Loss:811.6947\n",
      "[17000/20000] Loss:810.9167\n",
      "[17500/20000] Loss:809.9041\n",
      "[18000/20000] Loss:810.1392\n",
      "[18500/20000] Loss:808.8766\n",
      "[19000/20000] Loss:808.4343\n",
      "[19500/20000] Loss:812.4050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUC</td><td>0.91629</td></tr><tr><td>Training PRE</td><td>0.92828</td></tr><tr><td>Validation AUC</td><td>0.90553</td></tr><tr><td>Validation PRE</td><td>0.9159</td></tr><tr><td>training_loss</td><td>810.92249</td></tr><tr><td>val_loss</td><td>100.25244</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">autumn-sweep-100</strong>: <a href=\"https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749\" target=\"_blank\">https://wandb.ai/baron/Link_Prediction%40MLG/runs/ym48o749</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_234422-ym48o749/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "wandb.init(project='Link_Prediction@MLG', entity=\"baron\", group=\"GCN\")\n",
    "wandb.config[\"learning_rate\"] = 0.0001\n",
    "wandb.config[\"MAX_ITERATION\"] = 20000\n",
    "wandb.config[\"Dataset\"] = \"dataset3\"\n",
    "\n",
    "dataset = ReadData(wandb.config[\"Dataset\"])\n",
    "x = dataset.get_nodes_x()\n",
    "node_pairs = dataset.get_training_nodepairs()\n",
    "link_probs = gcn_model3.predict(x, node_pairs)\n",
    "# link_probs = link_logits.sigmoid()\n",
    "# neighbor feature\n",
    "training_array, target = dataset.get_training_data()\n",
    "training_data = torch.cat((torch.tensor(training_array, dtype=torch.float), link_probs.reshape(-1, 1)), 1)\n",
    "target = torch.Tensor(target.astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_data, target, test_size=0.1)\n",
    "gcn_nn_model3 = train(X_train, X_val, y_train, y_val)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b452f7-dea5-455a-bff0-c3baab60fb35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffe3de69-ac28-478e-bfe7-826159787015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f394998-1227-4f2a-84e3-a230fc380b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_train, y_val, y_train_hat, y_val_hat):\n",
    "    training_auc, val_auc = roc_auc_score(y_train, y_train_hat), roc_auc_score(y_val, y_val_hat)\n",
    "    training_pre, val_pre = average_precision_score(y_train, y_train_hat), average_precision_score(y_val, y_val_hat)\n",
    "    \n",
    "    return training_auc, training_pre, val_auc, val_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8904e08-48c7-427f-bbc2-dee03cc7d7a6",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f214d85-2dbd-46b7-b043-77e07e82a3b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02dd9ae3-bf99-4c69-acdf-784b17965f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2708/2708 [00:10<00:00, 257.52it/s]\n",
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2708/2708 [00:06<00:00, 440.61it/s]\n",
      "/home/baron/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing dataframe: \n",
      "          id    to  from     node_pair  ComNei  PreAtt  Jaccard     AA    Rwr\n",
      "2      E3964  2405  1765  (2405, 1765)       0       6    0.000  0.000  0.026\n",
      "1      E4849    81  1634    (81, 1634)       0       6    0.000  0.000  0.011\n",
      "1451   E5663  2104   593   (2104, 593)       0       0    0.000  0.000  0.000\n",
      "1445  E10013  2033    63    (2033, 63)       1       8    0.125  0.417  0.000\n",
      "1446   E3714  1130  2283  (1130, 2283)       0       6    0.000  0.000  0.000\n",
      "dataset1:\n",
      "       id          prob\n",
      "0  E10559  2.959970e-01\n",
      "1   E4849  1.000000e+00\n",
      "2   E3964  9.265349e-35\n",
      "3    E542  5.134942e-01\n",
      "4    E331  8.329121e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 3311/3311 [00:13<00:00, 247.55it/s]\n",
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 3311/3311 [00:06<00:00, 522.60it/s]\n",
      "/home/baron/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing dataframe: \n",
      "         id    to  from     node_pair  ComNei  PreAtt  Jaccard     AA    Rwr\n",
      "3     E5670  1063  1101  (1063, 1101)       2     468    0.024  1.135  0.024\n",
      "4     E5005  1067  1710  (1067, 1710)       0      14    0.000  0.000  0.009\n",
      "0     E3064  1315   586   (1315, 586)       0      14    0.000  0.000  0.000\n",
      "1254  E8203  1336    35    (1336, 35)       0       2    0.000  0.000  0.000\n",
      "1266  E6154   661   485    (661, 485)       0       3    0.000  0.000  0.000\n",
      "\n",
      "dataset2:\n",
      "      id      prob\n",
      "0  E3064  0.607592\n",
      "1   E298  0.587606\n",
      "2  E3512  1.000000\n",
      "3  E5670  1.000000\n",
      "4  E5005  0.997793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 876/876 [00:01<00:00, 520.16it/s]\n",
      "rwr scores..: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 876/876 [00:00<00:00, 899.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing dataframe: \n",
      "        id   to  from   node_pair  ComNei  PreAtt  Jaccard     AA    Rwr\n",
      "5    E1391  117   793  (117, 793)       1       4    0.333  0.721  0.016\n",
      "4    E2161  466   199  (466, 199)       0       1    0.000  0.000  0.008\n",
      "2    E3190  739   468  (739, 468)       0       8    0.000  0.000  0.001\n",
      "0     E370   26   317   (26, 317)       0       6    0.000  0.000  0.000\n",
      "434  E1511  522   773  (522, 773)       1       2    0.500  0.721  0.000\n",
      "\n",
      "dataset3:\n",
      "      id          prob\n",
      "0   E370  5.037491e-01\n",
      "1   E667  5.964093e-01\n",
      "2  E3190  2.294454e-13\n",
      "3   E848  4.648177e-01\n",
      "4  E2161  1.000000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/upload.csv\")\n",
    "dataset1 = ReadData(\"dataset1\")\n",
    "X_testing = dataset1.get_testing_data()\n",
    "X_testing = torch.tensor(X_testing).float()\n",
    "upload_csv[\"prob\"] = F.sigmoid(nn_model1(X_testing)).detach().numpy()\n",
    "print(\"dataset1:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_nn1_rwr.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset2/upload.csv\")\n",
    "dataset2 = ReadData(\"dataset2\")\n",
    "X_testing = dataset2.get_testing_data()\n",
    "X_testing = torch.tensor(X_testing).float()\n",
    "upload_csv[\"prob\"] = F.sigmoid(nn_model2(X_testing)).detach().numpy()\n",
    "print(\"\\ndataset2:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_nn2_rwr.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset3/upload.csv\")\n",
    "dataset3 = ReadData(\"dataset3\")\n",
    "X_testing = dataset3.get_testing_data()\n",
    "X_testing = torch.tensor(X_testing).float()\n",
    "upload_csv[\"prob\"] = F.sigmoid(nn_model3(X_testing)).detach().numpy()\n",
    "print(\"\\ndataset3:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_nn3_rwr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9dc38-0e7a-4f3b-909e-640830711c0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bbbaffc-0f81-46cc-916d-fe58443304f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "       id      prob\n",
      "0  E10559  0.517625\n",
      "1   E4849  0.426587\n",
      "2   E3964  0.426587\n",
      "3    E542  0.693317\n",
      "4    E331  0.920115\n",
      "\n",
      "dataset2:\n",
      "      id      prob\n",
      "0  E3064  0.572921\n",
      "1   E298  0.653814\n",
      "2  E3512  0.999594\n",
      "3  E5670  0.995861\n",
      "4  E5005  0.572921\n",
      "\n",
      "dataset3:\n",
      "      id      prob\n",
      "0   E370  0.477373\n",
      "1   E667  0.445013\n",
      "2  E3190  0.473666\n",
      "3   E848  0.520315\n",
      "4  E2161  0.413861\n"
     ]
    }
   ],
   "source": [
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/upload.csv\")\n",
    "X_testing = dataset1.get_testing_data()\n",
    "sum_value = 0\n",
    "for model in rf_models1:\n",
    "    predict_value = model.predict(X_testing)\n",
    "    sum_value += predict_value\n",
    "upload_csv[\"prob\"] = sum_value/len(rf_models1)\n",
    "print(\"dataset1:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_rf1_rwr.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset2/upload.csv\")\n",
    "X_testing = dataset2.get_testing_data()\n",
    "sum_value = 0\n",
    "for model in rf_models2:\n",
    "    predict_value = model.predict(X_testing)\n",
    "    sum_value += predict_value\n",
    "upload_csv[\"prob\"] = sum_value/len(rf_models2)\n",
    "print(\"\\ndataset2:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_rf2_rwr.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset3/upload.csv\")\n",
    "X_testing = dataset3.get_testing_data()\n",
    "sum_value = 0\n",
    "for model in rf_models3:\n",
    "    predict_value = model.predict(X_testing)\n",
    "    sum_value += predict_value\n",
    "upload_csv[\"prob\"] = sum_value/len(rf_models3)\n",
    "print(\"\\ndataset3:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_rf3_rwr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6e362-6c44-4ad9-866a-b2f278940e21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a1f33085-454f-4300-9024-ba93b2d848ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "       id      prob\n",
      "0  E10559  0.880797\n",
      "1   E4849  0.500000\n",
      "2   E3964  0.982014\n",
      "3    E542  0.731059\n",
      "4    E331  0.731059\n",
      "\n",
      "dataset2:\n",
      "      id      prob\n",
      "0  E3064  0.993307\n",
      "1   E298  0.999089\n",
      "2  E3512  0.952574\n",
      "3  E5670  0.999089\n",
      "4  E5005  0.731059\n",
      "\n",
      "dataset3:\n",
      "      id  prob\n",
      "0   E370   1.0\n",
      "1   E667   1.0\n",
      "2  E3190   1.0\n",
      "3   E848   1.0\n",
      "4  E2161   1.0\n"
     ]
    }
   ],
   "source": [
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/upload.csv\")\n",
    "dataset1 = ReadData(\"dataset1\")\n",
    "x = dataset1.get_nodes_x()\n",
    "test_node_pairs = dataset1.get_test_nodepairs()\n",
    "link_logits = gcn_model1.predict(x, test_node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "upload_csv[\"prob\"] = link_probs.detach().numpy()\n",
    "print(\"dataset1:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_gcn1_1000.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset2/upload.csv\")\n",
    "dataset2 = ReadData(\"dataset2\")\n",
    "x = dataset2.get_nodes_x()\n",
    "test_node_pairs = dataset2.get_test_nodepairs()\n",
    "link_logits = gcn_model2.predict(x, test_node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "upload_csv[\"prob\"] = link_probs.detach().numpy()\n",
    "print(\"\\ndataset2:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_gcn2_1000.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset3/upload.csv\")\n",
    "dataset3 = ReadData(\"dataset3\")\n",
    "x = dataset3.get_nodes_x()\n",
    "test_node_pairs = dataset3.get_test_nodepairs()\n",
    "link_logits = gcn_model3.predict(x, test_node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "upload_csv[\"prob\"] = link_probs.detach().numpy()\n",
    "print(\"\\ndataset3:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_gcn3_1000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7504f30-4c57-497c-be2e-1a843cfbf66f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN + Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "351ab0e0-11ca-41f8-805a-2d4c77d5317d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "       id      prob\n",
      "0  E10559  0.534469\n",
      "1   E4849  0.305609\n",
      "2   E3964  0.649014\n",
      "3    E542  0.568984\n",
      "4    E331  0.912752\n",
      "\n",
      "dataset2:\n",
      "      id      prob\n",
      "0  E3064  0.852497\n",
      "1   E298  0.897005\n",
      "2  E3512  0.988851\n",
      "3  E5670  0.997816\n",
      "4  E5005  0.373779\n",
      "\n",
      "dataset3:\n",
      "      id      prob\n",
      "0   E370  0.509860\n",
      "1   E667  0.501181\n",
      "2  E3190  0.496838\n",
      "3   E848  0.532785\n",
      "4  E2161  0.454266\n"
     ]
    }
   ],
   "source": [
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/upload.csv\")\n",
    "x = dataset1.get_nodes_x()\n",
    "test_node_pairs = dataset1.get_test_nodepairs()\n",
    "link_logits = gcn_model1.predict(x, test_node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "\n",
    "X_testing = dataset1.get_testing_data()\n",
    "X_testing = np.concatenate((X_testing, link_probs.cpu().detach().numpy().reshape(-1, 1)), axis=1)\n",
    "sum_value = 0\n",
    "for model in gcn_rf_models1:\n",
    "    predict_value = model.predict(X_testing)\n",
    "    sum_value += predict_value\n",
    "upload_csv[\"prob\"] = sum_value/len(gcn_rf_models1)\n",
    "print(\"dataset1:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_gcn_rf1.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset2/upload.csv\")\n",
    "x = dataset2.get_nodes_x()\n",
    "test_node_pairs = dataset2.get_test_nodepairs()\n",
    "link_logits = gcn_model2.predict(x, test_node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "\n",
    "X_testing = dataset2.get_testing_data()\n",
    "X_testing = np.concatenate((X_testing, link_probs.cpu().detach().numpy().reshape(-1, 1)), axis=1)\n",
    "sum_value = 0\n",
    "for model in gcn_rf_models2:\n",
    "    predict_value = model.predict(X_testing)\n",
    "    sum_value += predict_value\n",
    "upload_csv[\"prob\"] = sum_value/len(gcn_rf_models2)\n",
    "print(\"\\ndataset2:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_gcn_rf2.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset3/upload.csv\")\n",
    "x = dataset3.get_nodes_x()\n",
    "test_node_pairs = dataset3.get_test_nodepairs()\n",
    "link_logits = gcn_model3.predict(x, test_node_pairs)\n",
    "link_probs = link_logits.sigmoid()\n",
    "\n",
    "X_testing = dataset3.get_testing_data()\n",
    "X_testing = np.concatenate((X_testing, link_probs.cpu().detach().numpy().reshape(-1, 1)), axis=1)\n",
    "sum_value = 0\n",
    "for model in gcn_rf_models3:\n",
    "    predict_value = model.predict(X_testing)\n",
    "    sum_value += predict_value\n",
    "upload_csv[\"prob\"] = sum_value/len(gcn_rf_models3)\n",
    "print(\"\\ndataset3:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_gcn_rf3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e59b3-bdff-4947-b3a0-e10401423c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "2e82e958-37de-43eb-8fed-f05f7a5faad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:\n",
      "       id      prob\n",
      "0  E10559  0.426127\n",
      "1   E4849  0.033132\n",
      "2   E3964  0.650964\n",
      "3    E542  0.370743\n",
      "4    E331  0.958347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset2:\n",
      "      id      prob\n",
      "0  E3064  0.838953\n",
      "1   E298  0.929688\n",
      "2  E3512  1.000000\n",
      "3  E5670  1.000000\n",
      "4  E5005  0.393786\n",
      "\n",
      "dataset3:\n",
      "      id      prob\n",
      "0   E370  0.422646\n",
      "1   E667  0.772561\n",
      "2  E3190  0.501515\n",
      "3   E848  0.530014\n",
      "4  E2161  0.505471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baron/.local/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset1/upload.csv\")\n",
    "dataset1 = ReadData(\"dataset1\")\n",
    "x = dataset1.get_nodes_x()\n",
    "test_node_pairs = dataset1.get_test_nodepairs()\n",
    "link_probs = gcn_model1.predict(x, test_node_pairs)\n",
    "# link_probs = link_logits.sigmoid()\n",
    "# neighbor\n",
    "X_testing = dataset1.get_testing_data()\n",
    "X_testing = torch.cat((torch.tensor(X_testing, dtype=torch.float), link_probs.reshape(-1, 1)), 1)\n",
    "upload_csv[\"prob\"] = F.sigmoid(gcn_nn_model1(X_testing)).detach().numpy()\n",
    "print(\"dataset1:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_nn1_gcn_nosig.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset2/upload.csv\")\n",
    "dataset2 = ReadData(\"dataset2\")\n",
    "x = dataset2.get_nodes_x()\n",
    "test_node_pairs = dataset2.get_test_nodepairs()\n",
    "link_probs = gcn_model2.predict(x, test_node_pairs)\n",
    "# link_probs = link_logits.sigmoid()\n",
    "# neighbor\n",
    "X_testing = dataset2.get_testing_data()\n",
    "X_testing = torch.cat((torch.tensor(X_testing, dtype=torch.float), link_probs.reshape(-1, 1)), 1)\n",
    "upload_csv[\"prob\"] = F.sigmoid(gcn_nn_model2(X_testing)).detach().numpy()\n",
    "print(\"\\ndataset2:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_nn2_gcn_nosig.csv')\n",
    "\n",
    "upload_csv = pd.read_csv(\"../../data/SocialNetowrk/hw2_data/dataset3/upload.csv\")\n",
    "dataset3 = ReadData(\"dataset3\")\n",
    "x = dataset3.get_nodes_x()\n",
    "test_node_pairs = dataset3.get_test_nodepairs()\n",
    "link_probs = gcn_model3.predict(x, test_node_pairs)\n",
    "# link_probs = link_logits.sigmoid()\n",
    "# neighbor\n",
    "X_testing = dataset3.get_testing_data()\n",
    "X_testing = torch.cat((torch.tensor(X_testing, dtype=torch.float), link_probs.reshape(-1, 1)), 1)\n",
    "upload_csv[\"prob\"] = F.sigmoid(gcn_nn_model3(X_testing)).detach().numpy()\n",
    "print(\"\\ndataset3:\")\n",
    "print(upload_csv.head())\n",
    "upload_csv.to_csv('upload/upload_nn3_gcn_nosig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7305b-320d-4992-bcfc-45dfda0bdd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
